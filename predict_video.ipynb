{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **All necessary imports**"
      ],
      "metadata": {
        "id": "uUtUvVvcsXj5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fg3GzBmgCkKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ea9e91-65f7-4ff3-9ba1-fc6d889ab81a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget -qU\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import time\n",
        "import sys\n",
        "import torch\n",
        "import torchvision as tv\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as TF\n",
        "import matplotlib.pyplot as plt \n",
        "import os.path\n",
        "import wget\n",
        "\n",
        "from PIL import ImageDraw\n",
        "from google.colab import drive\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.datasets.vision import StandardTransform\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import transforms\n",
        "from datetime import timedelta\n",
        "from torchvision.models import resnet18\n",
        "from torch.optim import Adam, AdamW\n",
        "# from torch import nn can be removed?\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install wget\n",
        "\n",
        "DATA_DIR = \"./data\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **All variables**"
      ],
      "metadata": {
        "id": "GGNmLnWjtPeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Saving_frames_per_second = 30\n",
        "\n",
        "#choose on which video landmarks should be predicted\n",
        "Test_video_file = 'matthis_vid_v1.mp4'\n",
        "\n",
        "#choose which state dict to load\n",
        "Model_state = 'v6'\n",
        "\n",
        "#Insert params depending on state dict that will be loaded (needs to be the same)\n",
        "NET = \"ResNet18\"\n",
        "FC_LAYER = \"Lin-ReLu-Lin\""
      ],
      "metadata": {
        "id": "wNAv3NCEtVFL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **All used functions**"
      ],
      "metadata": {
        "id": "35GOTxOawdz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def device():\n",
        "    return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def add_fc(net, layer):\n",
        "    net.fc = layer\n",
        "    return net\n",
        "\n",
        "def plot_image(image: Image, labeling: np.ndarray=None):\n",
        "    try:\n",
        "      if isinstance(image, torch.Tensor) or isinstance(image, np.ndarray):\n",
        "          image = to_pil_image(image)\n",
        "    except TypeError:\n",
        "      pass\n",
        "    finally:\n",
        "      plt.imshow(image, interpolation='nearest', cmap='gray')\n",
        "      \n",
        "      if labeling is not None:\n",
        "          print(len(labeling))\n",
        "          for i in range(0, len(labeling)-1, 2):\n",
        "              plt.plot(labeling[i + 0],labeling[i + 1], marker=\".\", color='cyan')\n",
        "      plt.show()\n",
        "      \n",
        "def plot_predicted_facial_landmarks(*pil_images):\n",
        "  coordinates = predict_facial_landmarks(*pil_images)\n",
        "  for i in range(len(pil_images)):\n",
        "    plot_image(pil_images[i],coordinates[i])\n",
        "\n",
        "def return_predicted_facial_landmarks(*pil_images):\n",
        "  coordinates = predict_facial_landmarks(*pil_images)\n",
        "  for i in range(len(pil_images)):\n",
        "    plot_image(pil_images[i],coordinates[i])\n",
        "  \n",
        "\n",
        "def predict_facial_landmarks(*pil_images):\n",
        "  preprocess = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor()\n",
        "  ])\n",
        "\n",
        "  tensor_images = [preprocess(image.convert('L').convert('RGB')) for image in pil_images]\n",
        "  image_batch = torch.stack(tensor_images,dim=0).to(device())\n",
        "  net.eval()\n",
        "  labels = net(image_batch)\n",
        "  return labels.cpu().detach().numpy()\n",
        "\n",
        "def add_predictions(*pil_images):\n",
        "  coordinates = predict_facial_landmarks(*pil_images)\n",
        "  for i in range(len(pil_images)):\n",
        "    for j in range(0, len(coordinates[i]), 2):\n",
        "      radius = 1\n",
        "      leftUpPoint = (coordinates[i][j] - radius, coordinates[i][j+1] - radius)\n",
        "      rightDownPoint = (coordinates[i][j] + radius, coordinates[i][j+1] + radius)\n",
        "      draw = ImageDraw.Draw(pil_images[i])\n",
        "      draw.ellipse((leftUpPoint, rightDownPoint), fill = 'red')\n",
        "      #pil_images[i].putpixel( (coordinates[i][j], coordinates[i][j+1]), (255, 0, 0))\n",
        "  return pil_images\n",
        "\n",
        "def format_timedelta(td):\n",
        "    \"\"\"Utility function to format timedelta objects in a cool way (e.g 00:00:20.05) \n",
        "    omitting microseconds and retaining milliseconds\"\"\"\n",
        "    result = str(td)\n",
        "    try:\n",
        "        result, ms = result.split(\".\")\n",
        "    except ValueError:\n",
        "        return result + \".00\".replace(\":\", \"-\")\n",
        "    ms = int(ms)\n",
        "    ms = round(ms / 1e4)\n",
        "    return f\"{result}.{ms:02}\".replace(\":\", \"-\")\n",
        "\n",
        "\n",
        "def get_saving_frames_durations(cap, saving_fps):\n",
        "    \"\"\"A function that returns the list of durations where to save the frames\"\"\"\n",
        "    s = []\n",
        "    # get the clip duration by dividing number of frames by the number of frames per second\n",
        "    clip_duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS)\n",
        "    # use np.arange() to make floating-point steps\n",
        "    for i in np.arange(0, clip_duration, 1 / saving_fps):\n",
        "        s.append(i)\n",
        "    return s\n",
        "\n",
        "def extract_frames_from(video_file):\n",
        "  # read the video file    \n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "\n",
        "  # Check if camera opened successfully\n",
        "  if(cap.isOpened()== False):\n",
        "\t  print(\"Error opening video stream or file\")\n",
        "  # get the FPS of the video\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  # if the SAVING_FRAMES_PER_SECOND is above video FPS, then set it to FPS (as maximum)\n",
        "  saving_frames_per_second = min(fps, Saving_frames_per_second)\n",
        "  # get the list of duration spots to save\n",
        "  saving_frames_durations = get_saving_frames_durations(cap, saving_frames_per_second)\n",
        "  # start the loop\n",
        "  count = 0\n",
        "  frames = []\n",
        "  while True:\n",
        "      is_read, frame = cap.read()\n",
        "      if not is_read:\n",
        "          # break out of the loop if there are no frames to read\n",
        "          break\n",
        "      # get the duration by dividing the frame count by the FPS\n",
        "      frame_duration = count / fps\n",
        "      try:\n",
        "          # get the earliest duration to save\n",
        "          closest_duration = saving_frames_durations[0]\n",
        "      except IndexError:\n",
        "          # the list is empty, all duration frames were saved\n",
        "          break\n",
        "      if frame_duration >= closest_duration:\n",
        "          # if closest duration is less than or equals the frame duration, \n",
        "          # then save the frame\n",
        "          frame_duration_formatted = format_timedelta(timedelta(seconds=frame_duration))\n",
        "          #cv2.imwrite(os.path.join(filename, f\"frame{frame_duration_formatted}.jpg\"), frame) \n",
        "          ## drop the duration spot from the list, since this duration spot is already saved\n",
        "          frames.append(frame)\n",
        "          try:\n",
        "              saving_frames_durations.pop(0)\n",
        "          except IndexError:\n",
        "              pass\n",
        "      # increment the frame count\n",
        "      count += 1\n",
        "  return frames\n",
        "\n",
        "def plot_faces(images, coordinates=None, num=5):\n",
        "  for i in range(num):\n",
        "    if coordinates == None or coordinates[i] == None:\n",
        "      plot_face(images[i])\n",
        "    else:\n",
        "      plot_face(images[i],coordinates[i])\n",
        "\n",
        "\n",
        "def plot_face(image, coordinates=None, num=5):\n",
        "    plt.imshow(image, interpolation='nearest',cmap=\"gray\")\n",
        "    if coordinates is not None:\n",
        "      for i in range(0,len(coordinates)-1,2):\n",
        "        plt.plot(coordinates[i + 0],coordinates[i + 1],marker=\".\",color=\"red\")\n",
        "    plt.show()\n",
        "    \n",
        "def opencv_to_pil_image(opencv_image):\n",
        "  color_coverted = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2RGB)\n",
        "  pil_image=Image.fromarray(color_coverted)\n",
        "  return pil_image\n",
        "\n",
        "def pil_to_opencv_image(pil_image):\n",
        "  np_image=np.array(pil_image)  \n",
        "\n",
        "  # convert to a openCV2 image, notice the COLOR_RGB2BGR which means that \n",
        "  # the color is converted from RGB to BGR format\n",
        "  opencv_image=cv2.cvtColor(np_image, cv2.COLOR_RGB2BGR)\n",
        "  return opencv_image\n",
        "\n",
        "def opencv_images_to_video(opencv_images,video_filename):\n",
        "  height, width, layers = opencv_images[0].shape\n",
        "  image_size = (width, height)\n",
        "\n",
        "  out = cv2.VideoWriter(video_filename, cv2.VideoWriter_fourcc(*'MP4V'), Saving_frames_per_second, image_size)\n",
        "  for frame in opencv_images:\n",
        "    out.write(frame)\n",
        "  out.release()"
      ],
      "metadata": {
        "id": "xTpxd5kewiTA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fetching video and model state**"
      ],
      "metadata": {
        "id": "kbAF0YcEyrDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isfile(f\"./{Test_video_file}\"):\n",
        "  url = f\"https://github.com/ko-redtruck/facial-landmark-detection/raw/main/{Test_video_file}\"\n",
        "  wget.download(url, \".\")\n",
        "\n",
        "# if not os.path.isfile(f\"./{Model_state}\"):\n",
        "#   url = f\"https://github.com/ko-redtruck/facial-landmark-detection/raw/main/{Model_state}\"\n",
        "#   wget.download(url, \".\")"
      ],
      "metadata": {
        "id": "jzEfqUZUy0do"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining model to load state to**"
      ],
      "metadata": {
        "id": "uW8Jfb9xvWzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import artifact from wandb\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "wandb.init()\n",
        "artifact = wandb.use_artifact('leo-team/facial-landmark-detection/ResNet18:v14', type='model_state')\n",
        "artifact_path = artifact.download() + \"/model\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "bd09da09cc3b4b30a01f0d3f1b70bb48",
            "d9034f3b34ec48698941f865235198ee",
            "86e91ead348949a69f34fe082e2dd038",
            "f469c2b4ec2c4503ab8b10de2a28ed37",
            "fd5d0c288ca14754baa07a971d9fef54",
            "d27ef35d3c504cc0a12a2d4ed782d927",
            "65e552755e864a85bb56f0fa469f14f4",
            "5b465fc2fcf84aacbd581eb04b101219"
          ]
        },
        "id": "4Gkinft5P75T",
        "outputId": "482c7d0e-d7b9-4475-d951-bae0c6456ef8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:oq8vulkp) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd09da09cc3b4b30a01f0d3f1b70bb48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">dazzling-armadillo-2</strong>: <a href=\"https://wandb.ai/leonardhorns/uncategorized/runs/oq8vulkp\" target=\"_blank\">https://wandb.ai/leonardhorns/uncategorized/runs/oq8vulkp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220803_195757-oq8vulkp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:oq8vulkp). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220803_195940-mezgecat</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/leonardhorns/uncategorized/runs/mezgecat\" target=\"_blank\">proud-sunset-3</a></strong> to <a href=\"https://wandb.ai/leonardhorns/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter mappings\n",
        "fc_layers = {\n",
        "    \"Lin-ReLu-Lin\": nn.Sequential(\n",
        "        nn.Linear(512,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,30)\n",
        "    ),\n",
        "    \"Linear\": nn.Linear(512, 30)\n",
        "}\n",
        "\n",
        "networks = {\n",
        "    \"ResNet18\": add_fc(resnet18(pretrained=True), fc_layers[FC_LAYER])\n",
        "}\n",
        "\n",
        "\n",
        "net = networks[NET].to(device())\n",
        "\n",
        "#Load Pre-Trained Model\n",
        "net.load_state_dict(torch.load(artifact_path, map_location=device()))\n",
        "net.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbYAUQszyEdu",
        "outputId": "db01467e-049c-4502-ac5a-365a3bbfb8df"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=30, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predicting landmarks**"
      ],
      "metadata": {
        "id": "YwXczzr10H_9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvH1g9HlEWD9",
        "outputId": "d7bf1dad-d2fd-4b45-dd96-5c0b356c2a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE0D590>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAEE6590>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAEE6650>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAEE6E10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE72BD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8F110>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8F450>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8F050>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8F090>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAEE6950>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8FB10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAB7B7E50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8F290>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAB7FAE90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAADD2F90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE4D350>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8D890>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8D910>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8D990>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8DA10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8DA90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8DB10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8DB90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8DC10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8DC90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8DD10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8DD90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8DE10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8DE90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8DF10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8DF90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE8DFD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91390>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91310>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91290>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91210>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91190>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91110>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91090>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91450>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE914D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91550>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE915D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91650>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE916D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91750>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE917D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91850>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE918D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91950>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE919D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91A50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91AD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91B50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91BD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91C50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91CD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91D50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91DD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91E50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91ED0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91F50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91FD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE91410>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84110>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84190>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84210>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84290>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84310>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84390>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84410>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84490>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84510>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84590>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84610>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84690>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84710>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84790>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84810>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84890>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84910>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84990>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84A10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84A90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84B10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84B90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84C10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84C90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84D10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84D90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84E10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84E90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84F10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84F90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE84FD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE890D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89150>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE891D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89250>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE892D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89350>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE893D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89450>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE894D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89550>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE895D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89650>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE896D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89750>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE897D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89850>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE898D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89950>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE899D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89A50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89AD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89B50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89BD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89C50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89CD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89D50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89DD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89E50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89ED0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89F50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89FD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAE89050>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72110>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72190>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72210>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72290>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72310>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72390>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72410>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72490>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72510>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72590>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72610>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72690>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72710>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72790>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72810>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72890>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72910>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72990>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72A10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72A90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72B10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72B90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72C10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72C90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72D10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72D90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72E10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72E90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72F10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72F90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD72FD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD760D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD76150>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD761D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD76250>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD762D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD76350>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD763D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD76450>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD764D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD76550>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD765D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD76650>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD766D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD76750>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD767D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD76850>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD768D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD76950>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD769D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD76A50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDEAAD76AD0>)\n"
          ]
        }
      ],
      "source": [
        "  \n",
        "frames = [opencv_to_pil_image(opencv_image) for opencv_image in extract_frames_from(Test_video_file)]\n",
        "\n",
        "transform_image = TF.CenterCrop(224)\n",
        "frames = [transform_image(pil_image) for pil_image in frames]\n",
        "#plot_faces(frames,None,50)  \n",
        "#plot_predicted_facial_landmarks(*frames)\n",
        "frames = add_predictions(*frames)\n",
        "print(frames)\n",
        "\n",
        "opencv_images_to_video([pil_to_opencv_image(pil_image) for pil_image in frames],'video_predicted.mp4')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kbAF0YcEyrDY"
      ],
      "name": "predict_video.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd09da09cc3b4b30a01f0d3f1b70bb48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9034f3b34ec48698941f865235198ee",
              "IPY_MODEL_86e91ead348949a69f34fe082e2dd038"
            ],
            "layout": "IPY_MODEL_f469c2b4ec2c4503ab8b10de2a28ed37"
          }
        },
        "d9034f3b34ec48698941f865235198ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd5d0c288ca14754baa07a971d9fef54",
            "placeholder": "​",
            "style": "IPY_MODEL_d27ef35d3c504cc0a12a2d4ed782d927",
            "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "86e91ead348949a69f34fe082e2dd038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65e552755e864a85bb56f0fa469f14f4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b465fc2fcf84aacbd581eb04b101219",
            "value": 1
          }
        },
        "f469c2b4ec2c4503ab8b10de2a28ed37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd5d0c288ca14754baa07a971d9fef54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d27ef35d3c504cc0a12a2d4ed782d927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65e552755e864a85bb56f0fa469f14f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b465fc2fcf84aacbd581eb04b101219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}