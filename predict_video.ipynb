{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ko-redtruck/facial-landmark-detection/blob/main/predict_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **All necessary imports**"
      ],
      "metadata": {
        "id": "uUtUvVvcsXj5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fg3GzBmgCkKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88dc171b-9432-44f4-e43d-d01b940cd3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget -qU\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import time\n",
        "import sys\n",
        "import torch\n",
        "import torchvision as tv\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as TF\n",
        "import matplotlib.pyplot as plt \n",
        "import os.path\n",
        "import wget\n",
        "\n",
        "from PIL import ImageDraw\n",
        "from google.colab import drive\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.datasets.vision import StandardTransform\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import transforms\n",
        "from datetime import timedelta\n",
        "from torchvision.models import resnet18, resnet34\n",
        "from torch.optim import Adam, AdamW\n",
        "\n",
        "from scipy.ndimage.filters import uniform_filter1d\n",
        "# from torch import nn can be removed?\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install wget\n",
        "\n",
        "DATA_DIR = \"./data\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **All variables**"
      ],
      "metadata": {
        "id": "GGNmLnWjtPeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Saving_frames_per_second = 30\n",
        "\n",
        "#choose on which video landmarks should be predicted\n",
        "Test_video_file = 'matthis_vid_v1.mp4'\n",
        "\n",
        "#choose which state dict to load\n",
        "Model_state = 'v6'\n",
        "\n",
        "#Insert params depending on state dict that will be loaded (needs to be the same)\n",
        "NET = \"ResNet34\"\n",
        "FC_LAYER = \"Lin-ReLu-Lin\""
      ],
      "metadata": {
        "id": "wNAv3NCEtVFL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **All used functions**"
      ],
      "metadata": {
        "id": "35GOTxOawdz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def device():\n",
        "    return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def add_fc(net, layer):\n",
        "    net.fc = layer\n",
        "    return net\n",
        "\n",
        "def plot_image(image: Image, labeling: np.ndarray=None):\n",
        "    try:\n",
        "      if isinstance(image, torch.Tensor) or isinstance(image, np.ndarray):\n",
        "          image = to_pil_image(image)\n",
        "    except TypeError:\n",
        "      pass\n",
        "    finally:\n",
        "      plt.imshow(image, interpolation='nearest', cmap='gray')\n",
        "      \n",
        "      if labeling is not None:\n",
        "          print(len(labeling))\n",
        "          for i in range(0, len(labeling)-1, 2):\n",
        "              plt.plot(labeling[i + 0],labeling[i + 1], marker=\".\", color='cyan')\n",
        "      plt.show()\n",
        "      \n",
        "def plot_predicted_facial_landmarks(*pil_images):\n",
        "  coordinates = predict_facial_landmarks(*pil_images)\n",
        "  for i in range(len(pil_images)):\n",
        "    plot_image(pil_images[i],coordinates[i])\n",
        "\n",
        "def return_predicted_facial_landmarks(*pil_images):\n",
        "  coordinates = predict_facial_landmarks(*pil_images)\n",
        "  for i in range(len(pil_images)):\n",
        "    plot_image(pil_images[i],coordinates[i])\n",
        "  \n",
        "\n",
        "def predict_facial_landmarks(*pil_images):\n",
        "  preprocess = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor()\n",
        "  ])\n",
        "\n",
        "  tensor_images = [preprocess(image.convert('L').convert('RGB')) for image in pil_images]\n",
        "  image_batch = torch.stack(tensor_images,dim=0).to(device())\n",
        "  net.eval()\n",
        "  labels = net(image_batch)\n",
        "  return labels.cpu().detach().numpy()\n",
        "\n",
        "def smoothe_predictions(labels: np.ndarray, filter_size: int):\n",
        "  labels = labels.transpose(1, 0)\n",
        "  for i in range(len(labels)):\n",
        "    labels[i] = uniform_filter1d(labels[i], size=filter_size)\n",
        "  \n",
        "  return labels.transpose(1, 0)\n",
        "\n",
        "\n",
        "def add_predictions(*pil_images):\n",
        "  coordinates = predict_facial_landmarks(*pil_images)\n",
        "  coordinates = smoothe_predictions(coordinates, 10)\n",
        "\n",
        "  for i in range(len(pil_images)):\n",
        "    for j in range(0, len(coordinates[i]), 2):\n",
        "      radius = 1\n",
        "      leftUpPoint = (coordinates[i][j] - radius, coordinates[i][j+1] - radius)\n",
        "      rightDownPoint = (coordinates[i][j] + radius, coordinates[i][j+1] + radius)\n",
        "      draw = ImageDraw.Draw(pil_images[i])\n",
        "      draw.ellipse((leftUpPoint, rightDownPoint), fill = 'red')\n",
        "      #pil_images[i].putpixel( (coordinates[i][j], coordinates[i][j+1]), (255, 0, 0))\n",
        "  return pil_images\n",
        "\n",
        "def format_timedelta(td):\n",
        "    \"\"\"Utility function to format timedelta objects in a cool way (e.g 00:00:20.05) \n",
        "    omitting microseconds and retaining milliseconds\"\"\"\n",
        "    result = str(td)\n",
        "    try:\n",
        "        result, ms = result.split(\".\")\n",
        "    except ValueError:\n",
        "        return result + \".00\".replace(\":\", \"-\")\n",
        "    ms = int(ms)\n",
        "    ms = round(ms / 1e4)\n",
        "    return f\"{result}.{ms:02}\".replace(\":\", \"-\")\n",
        "\n",
        "\n",
        "def get_saving_frames_durations(cap, saving_fps):\n",
        "    \"\"\"A function that returns the list of durations where to save the frames\"\"\"\n",
        "    s = []\n",
        "    # get the clip duration by dividing number of frames by the number of frames per second\n",
        "    clip_duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS)\n",
        "    # use np.arange() to make floating-point steps\n",
        "    for i in np.arange(0, clip_duration, 1 / saving_fps):\n",
        "        s.append(i)\n",
        "    return s\n",
        "\n",
        "def extract_frames_from(video_file):\n",
        "  # read the video file    \n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "\n",
        "  # Check if camera opened successfully\n",
        "  if(cap.isOpened()== False):\n",
        "\t  print(\"Error opening video stream or file\")\n",
        "  # get the FPS of the video\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  # if the SAVING_FRAMES_PER_SECOND is above video FPS, then set it to FPS (as maximum)\n",
        "  saving_frames_per_second = min(fps, Saving_frames_per_second)\n",
        "  # get the list of duration spots to save\n",
        "  saving_frames_durations = get_saving_frames_durations(cap, saving_frames_per_second)\n",
        "  # start the loop\n",
        "  count = 0\n",
        "  frames = []\n",
        "  while True:\n",
        "      is_read, frame = cap.read()\n",
        "      if not is_read:\n",
        "          # break out of the loop if there are no frames to read\n",
        "          break\n",
        "      # get the duration by dividing the frame count by the FPS\n",
        "      frame_duration = count / fps\n",
        "      try:\n",
        "          # get the earliest duration to save\n",
        "          closest_duration = saving_frames_durations[0]\n",
        "      except IndexError:\n",
        "          # the list is empty, all duration frames were saved\n",
        "          break\n",
        "      if frame_duration >= closest_duration:\n",
        "          # if closest duration is less than or equals the frame duration, \n",
        "          # then save the frame\n",
        "          frame_duration_formatted = format_timedelta(timedelta(seconds=frame_duration))\n",
        "          #cv2.imwrite(os.path.join(filename, f\"frame{frame_duration_formatted}.jpg\"), frame) \n",
        "          ## drop the duration spot from the list, since this duration spot is already saved\n",
        "          frames.append(frame)\n",
        "          try:\n",
        "              saving_frames_durations.pop(0)\n",
        "          except IndexError:\n",
        "              pass\n",
        "      # increment the frame count\n",
        "      count += 1\n",
        "  return frames\n",
        "\n",
        "def plot_faces(images, coordinates=None, num=5):\n",
        "  for i in range(num):\n",
        "    if coordinates == None or coordinates[i] == None:\n",
        "      plot_face(images[i])\n",
        "    else:\n",
        "      plot_face(images[i],coordinates[i])\n",
        "\n",
        "\n",
        "def plot_face(image, coordinates=None, num=5):\n",
        "    plt.imshow(image, interpolation='nearest',cmap=\"gray\")\n",
        "    if coordinates is not None:\n",
        "      for i in range(0,len(coordinates)-1,2):\n",
        "        plt.plot(coordinates[i + 0],coordinates[i + 1],marker=\".\",color=\"red\")\n",
        "    plt.show()\n",
        "    \n",
        "def opencv_to_pil_image(opencv_image):\n",
        "  color_coverted = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2RGB)\n",
        "  pil_image=Image.fromarray(color_coverted)\n",
        "  return pil_image\n",
        "\n",
        "def pil_to_opencv_image(pil_image):\n",
        "  np_image=np.array(pil_image)  \n",
        "\n",
        "  # convert to a openCV2 image, notice the COLOR_RGB2BGR which means that \n",
        "  # the color is converted from RGB to BGR format\n",
        "  opencv_image=cv2.cvtColor(np_image, cv2.COLOR_RGB2BGR)\n",
        "  return opencv_image\n",
        "\n",
        "def opencv_images_to_video(opencv_images,video_filename):\n",
        "  height, width, layers = opencv_images[0].shape\n",
        "  image_size = (width, height)\n",
        "\n",
        "  out = cv2.VideoWriter(video_filename, cv2.VideoWriter_fourcc(*'MP4V'), Saving_frames_per_second, image_size)\n",
        "  for frame in opencv_images:\n",
        "    out.write(frame)\n",
        "  out.release()"
      ],
      "metadata": {
        "id": "xTpxd5kewiTA"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fetching video and model state**"
      ],
      "metadata": {
        "id": "kbAF0YcEyrDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isfile(f\"./{Test_video_file}\"):\n",
        "  url = f\"https://github.com/ko-redtruck/facial-landmark-detection/raw/main/{Test_video_file}\"\n",
        "  wget.download(url, \".\")\n",
        "\n",
        "# if not os.path.isfile(f\"./{Model_state}\"):\n",
        "#   url = f\"https://github.com/ko-redtruck/facial-landmark-detection/raw/main/{Model_state}\"\n",
        "#   wget.download(url, \".\")"
      ],
      "metadata": {
        "id": "jzEfqUZUy0do"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining model to load state to**"
      ],
      "metadata": {
        "id": "uW8Jfb9xvWzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import artifact from wandb\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "wandb.init()\n",
        "artifact = wandb.use_artifact('leo-team/facial-landmark-detection/{}:v5'.format(NET), type='model_state')\n",
        "artifact_path = artifact.download() + \"/model\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "f96f9b11f73d4189b5427fe79a00d0b8",
            "47429a17f7504eecb8da06999d021de4",
            "c9bbafa1420d43348dbcdf2bb3760072",
            "46bfefe2434c48ebb7acee65728f8e83",
            "e00ba76192b648249f42fa5e06704302",
            "5b5559758e664d25b3b2a7c2ce7de73a",
            "b5a835c2d5b143ebb7915eb5c409c5ec",
            "c1a3d72edf554672a01b7ca552ce58ed"
          ]
        },
        "id": "4Gkinft5P75T",
        "outputId": "33df636b-ccd0-4fff-c41e-b6271d8b5f18"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:1d9ncs8g) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f96f9b11f73d4189b5427fe79a00d0b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">elated-hill-23</strong>: <a href=\"https://wandb.ai/leonardhorns/uncategorized/runs/1d9ncs8g\" target=\"_blank\">https://wandb.ai/leonardhorns/uncategorized/runs/1d9ncs8g</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220805_184958-1d9ncs8g/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:1d9ncs8g). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220805_185120-393pwdf5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/leonardhorns/uncategorized/runs/393pwdf5\" target=\"_blank\">lemon-waterfall-24</a></strong> to <a href=\"https://wandb.ai/leonardhorns/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ResNet34:v5, 81.86MB. 1 files... Done. 0:0:0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter mappings\n",
        "fc_layers = {\n",
        "    \"Lin-ReLu-Lin\": nn.Sequential(\n",
        "        nn.Linear(512,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,30)#,\n",
        "        # nn.Linear(30, 8)\n",
        "    ),\n",
        "    \"Linear\": nn.Linear(512, 30)\n",
        "}\n",
        "\n",
        "networks = {\n",
        "    \"ResNet18\": add_fc(resnet18(pretrained=True), fc_layers[FC_LAYER]),\n",
        "    \"ResNet34\": add_fc(resnet34(weights=\"DEFAULT\"), fc_layers[FC_LAYER])\n",
        "}\n",
        "\n",
        "\n",
        "net = networks[NET].to(device())\n",
        "\n",
        "label_translation = [0, 1, 2, 3, 20, 21, 28, 29]\n",
        "#Load Pre-Trained Model\n",
        "state_dict = torch.load(artifact_path, map_location=device())\n",
        "# output_weights = torch.zeros(8, 30).to(device())\n",
        "# for i in range(len(label_translation)):\n",
        "#   output_weights[i, label_translation[i]] = 1.\n",
        "# state_dict['fc.3.weight'] = output_weights\n",
        "# state_dict['fc.3.bias'] = torch.zeros(8).to(device())\n",
        "# state_dict\n",
        "\n",
        "net.load_state_dict(state_dict)\n",
        "net.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbYAUQszyEdu",
        "outputId": "15a48417-fc41-4ce7-ab9c-c9bf050278dd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=30, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predicting landmarks**"
      ],
      "metadata": {
        "id": "YwXczzr10H_9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvH1g9HlEWD9",
        "outputId": "f2750ae3-8f14-4f2e-dc8b-8cd9d08fbb10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE6BCC50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDD9D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDDA50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDDAD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDDB50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDDC10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDDC90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDDD10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDDD90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDDDD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDDE50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDDED0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDDF50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDDFD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCCDD950>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82110>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82190>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82210>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82290>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82310>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82390>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82410>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82490>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82510>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82590>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82610>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82690>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82710>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82790>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82810>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82890>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82910>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82990>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82A10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82A90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82B10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82B90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82C10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82C90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82D10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82D90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82E10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82E90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82F10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82F90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCE82FD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD340D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34150>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD341D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34250>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD342D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34350>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD343D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34450>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD344D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34550>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD345D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34650>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD346D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34750>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD347D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34850>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD348D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34950>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD349D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34A50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34AD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34B50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34BD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34C50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34CD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34D50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34DD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34E50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DC76D150>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE6C78D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DD817ED0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DD817890>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DD817990>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DD817650>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DD817F10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DD8171D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DD817D50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DD817790>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DD817A50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34F10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34F90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD34FD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E0D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E150>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E1D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E250>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E2D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E350>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E3D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E450>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E4D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E550>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E5D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E650>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E6D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E750>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E7D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E850>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E8D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E950>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E9D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68EA50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68EAD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68EB50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68EBD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68EC50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68ECD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68ED50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68EDD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68EE50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68EED0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68EF50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68EFD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DE68E050>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82110>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82190>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82210>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82290>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82310>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82390>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82410>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82490>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82510>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82590>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82610>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82690>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82710>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82790>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82810>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82890>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82910>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82990>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82A10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82A90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82B10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82B90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82C10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82C90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82D10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82D90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82E10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82E90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82F10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82F90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD82FD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD830D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83150>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD831D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83250>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD832D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83350>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD833D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83450>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD834D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83550>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD835D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83650>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD836D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83750>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD837D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83850>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD838D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83950>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD839D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83A50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83AD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83B50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83BD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83C50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83CD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83D50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83DD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7F92DCD83E50>)\n"
          ]
        }
      ],
      "source": [
        "  \n",
        "frames = [opencv_to_pil_image(opencv_image) for opencv_image in extract_frames_from(Test_video_file)]\n",
        "\n",
        "transform_image = TF.CenterCrop(224)\n",
        "frames = [transform_image(pil_image) for pil_image in frames]\n",
        "#plot_faces(frames,None,50)  \n",
        "#plot_predicted_facial_landmarks(*frames)\n",
        "frames = add_predictions(*frames)\n",
        "print(frames)\n",
        "\n",
        "opencv_images_to_video([pil_to_opencv_image(pil_image) for pil_image in frames],'video_predicted.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OzZiKDCJiq4l"
      },
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kbAF0YcEyrDY"
      ],
      "name": "predict_video.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f96f9b11f73d4189b5427fe79a00d0b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47429a17f7504eecb8da06999d021de4",
              "IPY_MODEL_c9bbafa1420d43348dbcdf2bb3760072"
            ],
            "layout": "IPY_MODEL_46bfefe2434c48ebb7acee65728f8e83"
          }
        },
        "47429a17f7504eecb8da06999d021de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e00ba76192b648249f42fa5e06704302",
            "placeholder": "​",
            "style": "IPY_MODEL_5b5559758e664d25b3b2a7c2ce7de73a",
            "value": "0.020 MB of 0.020 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "c9bbafa1420d43348dbcdf2bb3760072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5a835c2d5b143ebb7915eb5c409c5ec",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1a3d72edf554672a01b7ca552ce58ed",
            "value": 1
          }
        },
        "46bfefe2434c48ebb7acee65728f8e83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e00ba76192b648249f42fa5e06704302": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b5559758e664d25b3b2a7c2ce7de73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5a835c2d5b143ebb7915eb5c409c5ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1a3d72edf554672a01b7ca552ce58ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}