{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **All necessary imports**"
      ],
      "metadata": {
        "id": "uUtUvVvcsXj5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "fg3GzBmgCkKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7168d119-c052-4bd2-b73a-28688e24d55f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import time\n",
        "import sys\n",
        "import torch\n",
        "import torchvision as tv\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as TF\n",
        "import matplotlib.pyplot as plt \n",
        "import os.path\n",
        "import wget\n",
        "\n",
        "from PIL import ImageDraw\n",
        "from google.colab import drive\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.datasets.vision import StandardTransform\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import transforms\n",
        "from datetime import timedelta\n",
        "from torchvision.models import resnet18\n",
        "from torch.optim import Adam, AdamW\n",
        "# from torch import nn can be removed?\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install wget\n",
        "\n",
        "DATA_DIR = \"./data\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **All variables**"
      ],
      "metadata": {
        "id": "GGNmLnWjtPeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Saving_frames_per_second = 30\n",
        "\n",
        "#choose on which video landmarks should be predicted\n",
        "Test_video_file = 'matthis_vid_v1.mp4'\n",
        "\n",
        "#choose which state dict to load\n",
        "Model_state = 'v6'\n",
        "\n",
        "#Insert params depending on state dict that will be loaded (needs to be the same)\n",
        "NET = \"ResNet18\"\n",
        "FC_LAYER = \"Lin-ReLu-Lin\""
      ],
      "metadata": {
        "id": "wNAv3NCEtVFL"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **All used functions**"
      ],
      "metadata": {
        "id": "35GOTxOawdz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def device():\n",
        "    return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def add_fc(net, layer):\n",
        "    net.fc = layer\n",
        "    return net\n",
        "\n",
        "def plot_image(image: Image, labeling: np.ndarray=None):\n",
        "    try:\n",
        "      if isinstance(image, torch.Tensor) or isinstance(image, np.ndarray):\n",
        "          image = to_pil_image(image)\n",
        "    except TypeError:\n",
        "      pass\n",
        "    finally:\n",
        "      plt.imshow(image, interpolation='nearest', cmap='gray')\n",
        "      \n",
        "      if labeling is not None:\n",
        "          print(len(labeling))\n",
        "          for i in range(0, len(labeling)-1, 2):\n",
        "              plt.plot(labeling[i + 0],labeling[i + 1], marker=\".\", color='cyan')\n",
        "      plt.show()\n",
        "      \n",
        "def plot_predicted_facial_landmarks(*pil_images):\n",
        "  coordinates = predict_facial_landmarks(*pil_images)\n",
        "  for i in range(len(pil_images)):\n",
        "    plot_image(pil_images[i],coordinates[i])\n",
        "\n",
        "def return_predicted_facial_landmarks(*pil_images):\n",
        "  coordinates = predict_facial_landmarks(*pil_images)\n",
        "  for i in range(len(pil_images)):\n",
        "    plot_image(pil_images[i],coordinates[i])\n",
        "  \n",
        "\n",
        "def predict_facial_landmarks(*pil_images):\n",
        "  preprocess = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor()\n",
        "  ])\n",
        "\n",
        "  tensor_images = [preprocess(image.convert('L').convert('RGB')) for image in pil_images]\n",
        "  image_batch = torch.stack(tensor_images,dim=0).to(device())\n",
        "  net.eval()\n",
        "  labels = net(image_batch)\n",
        "  return labels.cpu().detach().numpy()\n",
        "\n",
        "def add_predictions(*pil_images):\n",
        "  coordinates = predict_facial_landmarks(*pil_images)\n",
        "  for i in range(len(pil_images)):\n",
        "    for j in range(0, len(coordinates[i]), 2):\n",
        "      radius = 1\n",
        "      leftUpPoint = (coordinates[i][j] - radius, coordinates[i][j+1] - radius)\n",
        "      rightDownPoint = (coordinates[i][j] + radius, coordinates[i][j+1] + radius)\n",
        "      draw = ImageDraw.Draw(pil_images[i])\n",
        "      draw.ellipse((leftUpPoint, rightDownPoint), fill = 'red')\n",
        "      #pil_images[i].putpixel( (coordinates[i][j], coordinates[i][j+1]), (255, 0, 0))\n",
        "  return pil_images\n",
        "\n",
        "def format_timedelta(td):\n",
        "    \"\"\"Utility function to format timedelta objects in a cool way (e.g 00:00:20.05) \n",
        "    omitting microseconds and retaining milliseconds\"\"\"\n",
        "    result = str(td)\n",
        "    try:\n",
        "        result, ms = result.split(\".\")\n",
        "    except ValueError:\n",
        "        return result + \".00\".replace(\":\", \"-\")\n",
        "    ms = int(ms)\n",
        "    ms = round(ms / 1e4)\n",
        "    return f\"{result}.{ms:02}\".replace(\":\", \"-\")\n",
        "\n",
        "\n",
        "def get_saving_frames_durations(cap, saving_fps):\n",
        "    \"\"\"A function that returns the list of durations where to save the frames\"\"\"\n",
        "    s = []\n",
        "    # get the clip duration by dividing number of frames by the number of frames per second\n",
        "    clip_duration = cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS)\n",
        "    # use np.arange() to make floating-point steps\n",
        "    for i in np.arange(0, clip_duration, 1 / saving_fps):\n",
        "        s.append(i)\n",
        "    return s\n",
        "\n",
        "def extract_frames_from(video_file):\n",
        "  # read the video file    \n",
        "  cap = cv2.VideoCapture(video_file)\n",
        "\n",
        "  # Check if camera opened successfully\n",
        "  if(cap.isOpened()== False):\n",
        "\t  print(\"Error opening video stream or file\")\n",
        "  # get the FPS of the video\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  # if the SAVING_FRAMES_PER_SECOND is above video FPS, then set it to FPS (as maximum)\n",
        "  saving_frames_per_second = min(fps, Saving_frames_per_second)\n",
        "  # get the list of duration spots to save\n",
        "  saving_frames_durations = get_saving_frames_durations(cap, saving_frames_per_second)\n",
        "  # start the loop\n",
        "  count = 0\n",
        "  frames = []\n",
        "  while True:\n",
        "      is_read, frame = cap.read()\n",
        "      if not is_read:\n",
        "          # break out of the loop if there are no frames to read\n",
        "          break\n",
        "      # get the duration by dividing the frame count by the FPS\n",
        "      frame_duration = count / fps\n",
        "      try:\n",
        "          # get the earliest duration to save\n",
        "          closest_duration = saving_frames_durations[0]\n",
        "      except IndexError:\n",
        "          # the list is empty, all duration frames were saved\n",
        "          break\n",
        "      if frame_duration >= closest_duration:\n",
        "          # if closest duration is less than or equals the frame duration, \n",
        "          # then save the frame\n",
        "          frame_duration_formatted = format_timedelta(timedelta(seconds=frame_duration))\n",
        "          #cv2.imwrite(os.path.join(filename, f\"frame{frame_duration_formatted}.jpg\"), frame) \n",
        "          ## drop the duration spot from the list, since this duration spot is already saved\n",
        "          frames.append(frame)\n",
        "          try:\n",
        "              saving_frames_durations.pop(0)\n",
        "          except IndexError:\n",
        "              pass\n",
        "      # increment the frame count\n",
        "      count += 1\n",
        "  return frames\n",
        "\n",
        "def plot_faces(images, coordinates=None, num=5):\n",
        "  for i in range(num):\n",
        "    if coordinates == None or coordinates[i] == None:\n",
        "      plot_face(images[i])\n",
        "    else:\n",
        "      plot_face(images[i],coordinates[i])\n",
        "\n",
        "\n",
        "def plot_face(image, coordinates=None, num=5):\n",
        "    plt.imshow(image, interpolation='nearest',cmap=\"gray\")\n",
        "    if coordinates is not None:\n",
        "      for i in range(0,len(coordinates)-1,2):\n",
        "        plt.plot(coordinates[i + 0],coordinates[i + 1],marker=\".\",color=\"red\")\n",
        "    plt.show()\n",
        "    \n",
        "def opencv_to_pil_image(opencv_image):\n",
        "  color_coverted = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2RGB)\n",
        "  pil_image=Image.fromarray(color_coverted)\n",
        "  return pil_image\n",
        "\n",
        "def pil_to_opencv_image(pil_image):\n",
        "  np_image=np.array(pil_image)  \n",
        "\n",
        "  # convert to a openCV2 image, notice the COLOR_RGB2BGR which means that \n",
        "  # the color is converted from RGB to BGR format\n",
        "  opencv_image=cv2.cvtColor(np_image, cv2.COLOR_RGB2BGR)\n",
        "  return opencv_image\n",
        "\n",
        "def opencv_images_to_video(opencv_images,video_filename):\n",
        "  height, width, layers = opencv_images[0].shape\n",
        "  image_size = (width, height)\n",
        "\n",
        "  out = cv2.VideoWriter(video_filename, cv2.VideoWriter_fourcc(*'MP4V'), Saving_frames_per_second, image_size)\n",
        "  for frame in opencv_images:\n",
        "    out.write(frame)\n",
        "  out.release()"
      ],
      "metadata": {
        "id": "xTpxd5kewiTA"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fetching video and model state**"
      ],
      "metadata": {
        "id": "kbAF0YcEyrDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isfile(f\"./{Test_video_file}\"):\n",
        "  url = f\"https://github.com/ko-redtruck/facial-landmark-detection/raw/main/{Test_video_file}\"\n",
        "  wget.download(url, \".\")\n",
        "\n",
        "if not os.path.isfile(f\"./{Model_state}\"):\n",
        "  url = f\"https://github.com/ko-redtruck/facial-landmark-detection/raw/main/{Model_state}\"\n",
        "  wget.download(url, \".\")"
      ],
      "metadata": {
        "id": "jzEfqUZUy0do"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining model to load state to**"
      ],
      "metadata": {
        "id": "uW8Jfb9xvWzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter mappings\n",
        "fc_layers = {\n",
        "    \"Lin-ReLu-Lin\": nn.Sequential(\n",
        "        nn.Linear(512,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,30)\n",
        "    ),\n",
        "    \"Linear\": nn.Linear(512, 30)\n",
        "}\n",
        "\n",
        "networks = {\n",
        "    \"ResNet18\": add_fc(resnet18(pretrained=True), fc_layers[FC_LAYER])\n",
        "}\n",
        "\n",
        "\n",
        "net = networks[NET].to(device())\n",
        "\n",
        "#Load Pre-Trained Model\n",
        "net.load_state_dict(torch.load('/content/v6'))\n",
        "net.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbYAUQszyEdu",
        "outputId": "9e88b637-ca4a-4ff1-a0f1-2fcc53286296"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=30, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predicting landmarks**"
      ],
      "metadata": {
        "id": "YwXczzr10H_9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvH1g9HlEWD9",
        "outputId": "d7f54e15-009c-4b7d-e94f-dcd20982f599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<PIL.Image.Image image mode=RGB size=224x224 at 0x7FDDF0FF9C10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDDF10D47D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDDF0FCDBD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E177A90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1770D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1779D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E177250>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E177A10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E177950>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E177910>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1774D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E177750>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E177510>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E177610>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E177B50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E177F50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E177B10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E177E10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E170510>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E177C10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C150>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C910>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06CC90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C5D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C250>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C450>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C1D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06CA10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C710>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06CED0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C990>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C490>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C610>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06CD90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06CF10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C690>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C750>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C9D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C110>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06CF50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C190>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06CE10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C850>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C2D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C550>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E06C590>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1E1750>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1E1D90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1E1A10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1E19D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1E1A90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1E1290>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1E1890>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1E1190>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1E1E10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1E1810>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDDF0C9E810>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDDF0C9E650>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8C28B910>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E02CB50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE46211B10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8C5E8990>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDF23479F10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE46700490>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE46700C90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE467008D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE46700590>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE46700510>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE46700E90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE46700450>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE46700350>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE467003D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE467009D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE46700D10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8BAF6090>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE46700F90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8A2B8F50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE4663CE50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8A1146D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8A114150>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8A1142D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8A114E90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8A114250>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038EFD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038EA10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038E3D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038E310>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038E350>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038EED0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038E2D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038ED90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038E990>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038E250>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038E5D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038E150>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038EB50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038ECD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038E6D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038E790>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038EE50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038E710>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038EB90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038EF50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038ED50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038E190>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038E550>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDF1565E7D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE8038EB10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193550>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193B90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193690>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193490>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193A50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193750>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1936D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193790>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193E50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193D90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193F10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193450>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1935D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193050>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1933D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193850>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193B50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193710>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193D10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193610>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193E90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193510>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193910>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193250>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193E10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193C90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1937D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193CD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E193350>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE467F7E90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE2E1939D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE80068790>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE80068990>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE800685D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE80068DD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE80068890>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE80068A90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE80068A50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE80068650>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE80068110>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE80068310>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE80068610>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE80068950>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7ED0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7350>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7490>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7D50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7C10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B70D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7F90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7390>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7AD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B77D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7E10>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7850>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7190>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7BD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B72D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7990>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B79D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7C90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7CD0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7E50>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B78D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B7610>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDE802B73D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDDF0D70C90>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDDF0D708D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDDF10318D0>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDDF1031210>, <PIL.Image.Image image mode=RGB size=224x224 at 0x7FDDF1031CD0>)\n"
          ]
        }
      ],
      "source": [
        "  \n",
        "frames = [opencv_to_pil_image(opencv_image) for opencv_image in extract_frames_from(Test_video_file)]\n",
        "\n",
        "transform_image = TF.CenterCrop(224)\n",
        "frames = [transform_image(pil_image) for pil_image in frames]\n",
        "#plot_faces(frames,None,50)  \n",
        "#plot_predicted_facial_landmarks(*frames)\n",
        "frames = add_predictions(*frames)\n",
        "print(frames)\n",
        "\n",
        "opencv_images_to_video([pil_to_opencv_image(pil_image) for pil_image in frames],'video_predicted.mp4')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kbAF0YcEyrDY"
      ],
      "name": "read_video.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}